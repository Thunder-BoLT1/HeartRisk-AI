# -*- coding: utf-8 -*-
"""HeartDiseaseProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CkY1952Z5mYze3ANO80u5nV4D2SgsPdk

Mazen Ahmed Data Processing
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from prince import MCA
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import f_classif
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay

df = pd.read_csv('/content/Heart_disease_statlog.csv')
df.head()

X, Y = df.iloc[:, :-1], df.iloc[:, -1]
X.head()

X.info()

#Becasue That starts with 1-2-3 we subtract 1 to start classes with 0-1-2
print(X["thal"].unique())
X["thal"] -= 1
print(X["thal"].unique())
X.head()

#Seprate the features into continues and categorical features for proper correlation
X_Continues = X.drop(["sex", "cp", "fbs", "restecg", "exang", "slope", "ca", "thal"], axis=1)
X_Categorical = X[["sex", "cp", "fbs", "restecg", "exang", "slope", "ca", "thal"]]

#Correlation of Continues features with each other
corr_matrix = X_Continues.corr()
plt.figure(figsize=(10, 5))
sns.heatmap(corr_matrix, annot=True)
plt.show()

#Correlation of Continues features with the target
corr_matrix_features = np.triu(corr_matrix, k = 1)
i, *_ = np.where(abs(corr_matrix_features) >= 0.8)
i = np.unique(i)
print(f"Number of Removed (Correlated) Features = {len(i)}")
X_Continues = X_Continues.drop(df.columns[i], axis=1)
X_Continues.head()

F_values, P_values = f_classif(X_Continues, Y)
i, *_ = np.where(P_values > 0.06)
print(f"Number of Removed (Correlated) Features = {len(i)}")
X_Continues = X_Continues.drop(X_Continues.columns[i], axis=1)
X_Continues.head()

#Correlation of Categricol Features With each other
from scipy.stats import chi2_contingency

def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    return np.sqrt(phi2 / min(k-1, r-1))

corr_matrix = pd.DataFrame(index=X_Categorical.columns, columns=X_Categorical.columns)

for col1 in X_Categorical.columns:
    for col2 in X_Categorical.columns:
       corr_matrix.loc[col1, col2] = cramers_v(X_Categorical[col1], X_Categorical[col2])
corr_matrix = corr_matrix.astype(float)
sns.heatmap(corr_matrix, annot=True)
plt.show()

corr_matrix_features = np.triu(corr_matrix, k = 1)
i, *_ = np.where(abs(corr_matrix_features) >= 0.7)
print(f"Number of Removed (Correlated) Features = {len(i)}")
X_Categorical = X_Categorical.drop(X_Categorical.columns[i], axis=1)
X_Categorical.head()

# Correlation of Categricol Features With the Target
corr_matrix = pd.DataFrame(index=X_Categorical.columns, columns=["Target"])

for col in X_Categorical.columns:
    corr_matrix.loc[col, "Target"] = cramers_v(X_Categorical[col], Y)
corr_matrix = corr_matrix.astype(float)

i, *_ = np.where(corr_matrix["Target"] < 0.2)
print(f"Number of Removed (Correlated) Features = {len(i)}")
X_Categorical = X_Categorical.drop(X_Categorical.columns[i], axis=1)
X_Categorical.head()

#Standrization of Continues Features
scaler = StandardScaler()
X_Continues = pd.DataFrame(scaler.fit_transform(X_Continues), columns=X_Continues.columns)
X_Continues.head()

#PCA for Continues Feature Reduction
pca = PCA(n_components=0.95) # Capture PC with variance of 95% of total variance
X_Continues = pd.DataFrame(pca.fit_transform(X_Continues), columns=[f"PC{i+1}" for i in range(len(pca.components_))])
X_Continues.head()

#MCA for Categorical Feature Reduction
#mca = MCA(n_components=6)
#X_Categorical = mca.fit_transform(X_Categorical)
#X_Categorical.columns = [f"PC{i+1}" for i in range(len(X_Categorical.columns))]
#X_Categorical.head()

# Reassign X to the new Columns after removing insignificant Features
X = pd.concat([X_Continues, X_Categorical], axis=1)
print(X.shape)
X.head()

#92.59%  in Logistic Reg and SVC(rbf, c=10, gamma=0.001)
#90.74% Gaussian Naive Bayes
#88 with random forest

#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
#model = LogisticRegression()
#model.fit(X_train, Y_train)
#y_pred = model.predict(X_test)
#print(f"Model Accuracy = {accuracy_score(Y_test, y_pred):.2%}")

"""Jana Visualization"""

df = pd.concat([X, Y], axis=1)
df

df.shape

df.info()

df.isnull().sum()

df.duplicated().sum()

df.describe()

plt.figure(figsize=(6,4))
sns.countplot(x='target', data=df)
plt.title('Heart Disease Distribution')
plt.xlabel('0 = No Disease, 1 = Disease')
plt.ylabel('Count')
plt.show()

n_cols = len(X_Continues.columns)

n_rows = int(np.ceil(n_cols / 3))

fig, axes = plt.subplots(n_rows, 3, figsize=(15, 4 * n_rows))
axes = axes.flatten()

for i, col in enumerate(X_Continues.columns):
    sns.histplot(df[col], kde=True, bins=20, ax=axes[i])
    axes[i].set_title(f'Distribution of {col}')

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

n_cols = len(X_Continues.columns)
n_rows = int(np.ceil(n_cols / 3))

fig, axes = plt.subplots(n_rows, 3, figsize=(15, 4 * n_rows))
axes = axes.flatten()

for i, col in enumerate(X_Continues.columns):
    sns.boxplot(x=df[col], ax=axes[i])
    axes[i].set_title(f'Boxplot of {col}')

# Remove unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

numeric_cols = X_Continues.columns

for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5*IQR
    upper = Q3 + 1.5*IQR
    outliers = df[(df[col] < lower) | (df[col] > upper)]
    print(f"{col} has {len(outliers)} outliers")
    # Remove/Cap Outliers (capping)
    #df[col] = df[col].clip(lower, upper)

n_cols = len(X_Continues.columns)
n_rows = int(np.ceil(n_cols / 3))

fig, axes = plt.subplots(n_rows, 3, figsize=(15, 4 * n_rows))
axes = axes.flatten()

for i, col in enumerate(X_Continues.columns):
    sns.boxplot(x=df[col], ax=axes[i])
    axes[i].set_title(f'Boxplot of {col}')

# Remove unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

n_cols = len(X_Categorical.columns)
n_rows = int(np.ceil(n_cols / 3))

fig, axes = plt.subplots(n_rows, 3, figsize=(15, 4 * n_rows))
axes = axes.flatten()

for i, col in enumerate(X_Categorical.columns):
    sns.countplot(x=col, data=df, ax=axes[i])
    axes[i].set_title(f'Count of {col}')
    axes[i].tick_params(axis='x', rotation=45)

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""Mazen Ahmed Models"""

X = df.drop('target', axis=1)
Y = df['target']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

ModelsEval = {}

"""LogesticRegression"""

LogisticRegModel = LogisticRegression()

LogisticRegModel.fit(X_train, Y_train)

y_pred = LogisticRegModel.predict(X_test)
accuracy = accuracy_score(Y_test, y_pred)

cnf_matrix = confusion_matrix(Y_test, y_pred)

ModelsEval['LogisticRegression'] = (accuracy, confusion_matrix(Y_test, y_pred), LogisticRegModel)

"""SVM"""

maxAccuracy = 0
for kernel in ['linear', 'poly', 'rbf']:
  for c in [0.1, 1, 10, 100]:
    for gamma in [0.001, 0.01, 0.1, 1]:
      SVMModel = SVC(kernel=kernel, C=c, gamma=gamma)
      SVMModel.fit(X_train, Y_train)
      y_pred = SVMModel.predict(X_test)
      accuracy = accuracy_score(Y_test, y_pred)
      if(accuracy > maxAccuracy):
        maxAccuracy = accuracy
        cnf_matrix = confusion_matrix(Y_test, y_pred)
        ModelsEval['SVM'] = (accuracy, confusion_matrix(Y_test, y_pred), SVMModel)

"""KNN"""

maxAccuracy = 0
for k in range(1, 21):
  KNNModel = KNeighborsClassifier(n_neighbors=k)
  KNNModel.fit(X_train, Y_train)
  y_pred = KNNModel.predict(X_test)
  accuracy = accuracy_score(Y_test, y_pred)
  if(accuracy > maxAccuracy):
    maxAccuracy = accuracy
    cnf_matrix = confusion_matrix(Y_test, y_pred)
    ModelsEval['KNN'] = (accuracy, confusion_matrix(Y_test, y_pred), KNNModel)

"""DecisionTree"""

maxAccuracy = 0
for depth in range(1, 21):
  for criterion in ['gini', 'entropy']:
    DecisionTreeModel = DecisionTreeClassifier(criterion=criterion, max_depth=depth)
    DecisionTreeModel.fit(X_train, Y_train)
    y_pred = KNNModel.predict(X_test)
    accuracy = accuracy_score(Y_test, y_pred)
    if(accuracy > maxAccuracy):
      maxAccuracy = accuracy
      cnf_matrix = confusion_matrix(Y_test, y_pred)
      ModelsEval['DT'] = (accuracy, confusion_matrix(Y_test, y_pred), DecisionTreeModel)

"""Naive Bayes"""

NaiveBayesModel = GaussianNB()
NaiveBayesModel.fit(X_train, Y_train)
y_pred = NaiveBayesModel.predict(X_test)
accuracy = accuracy_score(Y_test, y_pred)
cnf_matrix = confusion_matrix(Y_test, y_pred)
ModelsEval['NaiveBayes'] = (accuracy, confusion_matrix(Y_test, y_pred), NaiveBayesModel)

"""Models Comparisons"""

n_cols = len(ModelsEval.keys())
n_rows = int(np.ceil(n_cols / 3))

fig, axes = plt.subplots(n_rows, 3, figsize=(15, 4 * n_rows))
axes = axes.flatten()

for i, model in enumerate(ModelsEval.keys()):
    disp = ConfusionMatrixDisplay(confusion_matrix=ModelsEval[model][1], display_labels=["Normal", "Risk"])
    disp.plot(ax=axes[i], cmap='Blues')
    axes[i].set_title(f'Confusion Matrix of {model}')

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(ModelsEval.keys(), [ModelsEval[model][0]*100 for model in ModelsEval.keys()], marker="o", linestyle="--")
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Model Accuracy Comparison')
plt.show()

"""**SVM & Logistic Regression** Have the Same Accuracy of 92.59% But we will **consider the Logestic Regression Model as it has Less False Positive** and this is very important in medical **Fields**"""

filename = 'finalized_model.pkl'
with open(filename, 'wb') as file:
    pickle.dump(ModelsEval['LogisticRegression'][0], file)
    pickle.dump(ModelsEval['LogisticRegression'][2], file)
    pickle.dump(scaler, file)
    pickle.dump(pca, file)